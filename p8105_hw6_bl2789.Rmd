---
title: "Homework 6"
author: "Bingkun Luo"
date: "11/22/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(faraway)
library(modelr)

```

#### Problem 1
##### a

```{r}
birth_weight = read_csv("./data/birthweight.csv")%>%
                mutate(babysex = as.factor(babysex),
                       frace = as.factor(frace),
                       malform = as.factor(malform),
                       mrace = as.factor(mrace))

str(birth_weight)
```

Loaded and checked there is `r sum(is.na(birth_weight))` NAs in the imported dataset for the regression analysis.

##### b
Backward elimination example using step, 
Fristly remove the entire categorical variable, since it doesnot make much sense if the model selection only retain some of the  multiple levels.
```{r}
mod = lm(bwt~.-babysex-frace-mrace,data = birth_weight)
back = step(mod,direction = c("backward")) 
summary(back)
```


The model I selected by Backward selection based on AIC is 

```{r}
fit = lm(bwt~bhead+blength+delwt+fincome+gaweeks+menarche+mheight+momage+parity+ppwt+smoken,data = birth_weight)
fit
summary(fit)
```



Plot:

```{r}

plot = as_tibble(add_residuals(fit,data=add_predictions(fit,data=birth_weight)))
ggplot(plot, aes(x=pred,y=resid))+
  geom_point(alpha = 0.4)+
  stat_smooth(method = "lm") +
  labs(
    title = "The model residuals against fitted values",
    x = "Prediction",
    y = "Residuals",
    caption = "Data from birth weight")
                  

  

```


The Residual Plot seems like a flat line by 0, however it is residual varies too much from -1000 to 2000. Residuals are not symmetrically distributed, tending to cluster towards the right of the plot.


Other two models:

```{r}
fit_1 = lm(bwt~blength+gaweeks,data = birth_weight)
fit_2 = lm(bwt~bhead*blength*babysex,data = birth_weight)

```

Cross validation:

```{r}

set.seed(1)
cv_df = 
  crossv_mc(birth_weight,100) 
cv_df =
  cv_df %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))
```


```{r}
cv_df = 
  cv_df %>% 
  mutate(fit = map(train,~lm(bwt~bhead+blength 
        +delwt+fincome+gaweeks+menarche+mheight+momage+parity+ppwt+smoken,data = .)),
        fit_1 = map(train,~lm(bwt~blength+gaweeks,data = .)),
        fit_2 = map(train,~lm(bwt~bhead*blength*babysex,data = .))) %>% 
  mutate(rmse_fit = map2_dbl(fit, test, ~rmse(model = .x, data = .y)),
         rmse_fit_1 = map2_dbl(fit_1, test, ~rmse(model = .x, data = .y)),
         rmse_fit_2 = map2_dbl(fit_2, test, ~rmse(model = .x, data = .y)))
```



```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model))%>%
  group_by(model)%>%
  summarise(mean_rmse = mean(rmse))%>%
  pivot_wider(names_from = model,values_from = mean_rmse)%>%
  kableExtra::kable(type = "markdown")
  
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model))%>%
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```


Since the first model, which we selected using stepwise backward selection according to AIC score, has the lowest cross-validation predicted error for mean and overall distribution. Our model is a better choice.

#### Problem 2
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

##### a

Simple linear regression

```{r}

set.seed(233)
boot_straps = weather_df %>% 
              modelr::bootstrap(n = 5000)%>%
              mutate(
                fitted =map(strap,~lm(tmax~tmin,data = .x)),
                results1 = map(fitted,broom::tidy),
                results2 = map(fitted,broom::glance))%>%
              select(-strap, -fitted)
 
```

two estimates:

```{r}
 
r_square = boot_straps %>%
              unnest(results2)%>%
              select(.id,r.squared)%>%
              janitor::clean_names()

             
log = boot_straps %>%
      unnest(results1)%>%
      select(.id, term, estimate)%>%
      pivot_wider(names_from = "term",values_from = "estimate")%>% 
      janitor::clean_names() %>% 
      mutate(log = log(intercept * tmin))%>%
      select(id,log)

final = left_join(r_square,log)
ggplot(final)+
   geom_density(aes(x = r_squared),fill = "red",alpha = 0.4)+
  labs(
    title = " Distribution of estimated R squared",
    x = "R squared",
    y = "Density",
    caption = "Data from boot straps")
  
ggplot(final)+
   geom_density(aes(x = log),fill = "sky blue",alpha = 0.7)+
    labs(
    title = " Distribution of estimated log(β_0*β_1)",
    x = "log(β_0*β_1)",
    y = "Density",
    caption = "Data from boot straps")
                  
  


              
```

* The R^2 is pretty large for the boot straps data and has the tmin could explain decently for tmax in the simple linear regression model.  
* The log(β_0*β_1) estimates' distribution followed the bell shape, which implies the validation of normality.



##### b

Identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval

* **R_squared**
```{r}
# R_squared
c(quantile(pull(final,r_squared), probs = 0.025), quantile(pull(final,r_squared), probs = 0.975))
```

* **log(β_0*β_1)**
```{r}
c(quantile(pull(final,log), probs = 0.025), quantile(pull(final,log), probs = 0.975))
```










